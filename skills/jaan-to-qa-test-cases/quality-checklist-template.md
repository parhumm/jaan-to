# Quality Checklist: {{title}} Test Cases

> Generated by jaan.to | {{date}}

---

## 10-Point Peer Review Checklist

From ISTQB standards and research best practices:

### 1. Alignment ‚òê
- [ ] Every test maps directly to a requirement or acceptance criterion
- [ ] No orphaned tests without clear purpose
- [ ] Traceability tags present (@REQ-{id})
- [ ] Coverage matrix shows bidirectional traceability

**Status**: {{alignment_status}}

### 2. Clarity ‚òê
- [ ] Steps are unambiguous with specific UI elements
- [ ] No vague language ("works", "properly", "correctly")
- [ ] Exact button names, field labels, error messages
- [ ] Element identifiers (IDs, classes) where applicable

**Example**:
- ‚ùå "Click the button" ‚Üí ‚úÖ "Click the 'Submit Order' button"
- ‚ùå "Verify system works" ‚Üí ‚úÖ "Verify order confirmation displays order ID #12345"

**Status**: {{clarity_status}}

### 3. Completeness ‚òê
- [ ] Preconditions documented for every scenario
- [ ] Test data provided for every step
- [ ] Expected results for every action
- [ ] All test types present: positive, negative, boundary, edge

**Coverage Distribution**:
- Positive: {{positive_pct}}% (target: 30%)
- Negative: {{negative_pct}}% (target: 40%)
- Edge: {{edge_pct}}% (target: 30%)

**Status**: {{completeness_status}}

### 4. Measurable Expected Results ‚òê
- [ ] Specific outcomes with numeric thresholds
- [ ] Exact error text instead of generic descriptions
- [ ] Performance thresholds where applicable (e.g., "<3 seconds")
- [ ] Verifiable conditions (not subjective)

**Example**:
- ‚ùå "Page loads quickly" ‚Üí ‚úÖ "Page loads in <3 seconds"
- ‚ùå "Error displays" ‚Üí ‚úÖ "Error message 'Email is required' displays"

**Status**: {{measurable_status}}

### 5. Test Data Quality ‚òê
- [ ] Explicit values: "test@example.com" not "[valid email]"
- [ ] Realistic data: "ValidP@ss123!" not "[password]"
- [ ] No placeholders or generic references
- [ ] Reproducible values (absolute dates, not "yesterday")

**Standard Test Data Used**:
- Emails: test@example.com, invalid@test.com, user+test@example.com
- Passwords: ValidP@ss123!, weak, tooshort1, 123456
- Dates: 2024-01-15, 2024-12-31, 2024-02-29
- Numbers: 0, 1, 50, 99, 100, 101, -1

**Status**: {{test_data_status}}

### 6. Traceability ‚òê
- [ ] Linked to requirement IDs via @REQ-{id} tags
- [ ] Bidirectional traceability (AC ‚Üí Tests, Tests ‚Üí AC)
- [ ] Coverage matrix included
- [ ] No broken references

**Traceability Matrix**: {{traceability_count}} requirements covered

**Status**: {{traceability_status}}

### 7. Independence ‚òê
- [ ] Tests run without dependencies on other tests
- [ ] Setup/teardown explicit in preconditions/postconditions
- [ ] No hidden state assumptions
- [ ] Tests can execute in any order

**Example**:
- ‚ùå "Assumes user already logged in from previous test"
- ‚úÖ "Given I am logged in as 'test@example.com'" (explicit precondition)

**Status**: {{independence_status}}

### 8. Reproducibility ‚òê
- [ ] Any tester can execute identically
- [ ] Absolute references (not relative: "yesterday" ‚Üí "2024-01-15")
- [ ] All entities defined in preconditions
- [ ] Environment requirements documented

**Status**: {{reproducibility_status}}

### 9. Negative Coverage ‚òê
- [ ] Invalid inputs tested (malformed, out-of-range)
- [ ] Error scenarios covered (timeouts, failures)
- [ ] Minimum 30% negative test distribution
- [ ] Recovery paths validated

**Negative Test Count**: {{negative_count}} / {{total_tests}} = {{negative_pct}}%

**Status**: {{negative_coverage_status}}

### 10. Edge Coverage ‚òê
- [ ] Boundary conditions addressed (min/max, limits)
- [ ] 5 priority categories represented:
  - [ ] Empty/Null States ({{empty_count}} tests)
  - [ ] Boundary Values ({{boundary_count}} tests)
  - [ ] Error Conditions ({{error_count}} tests)
  - [ ] Concurrent Operations ({{concurrent_count}} tests)
  - [ ] State Transitions ({{state_count}} tests)

**Status**: {{edge_coverage_status}}

---

**OVERALL SCORE: {{checklist_score}}/10** (Pass: 8+)

**Result**: {{pass_fail}}

---

## Anti-Patterns to Reject

| Anti-Pattern | Example | Fix | Severity |
|--------------|---------|-----|----------|
| **Vague steps** | "Verify system works correctly" | "Verify order confirmation displays order ID #12345" | High |
| **Missing preconditions** | Starts with "When I click Submit" | Add: "Given I am logged in as 'test@example.com'" | Critical |
| **Untestable result** | "System should be fast" | "Page loads within 3 seconds" | High |
| **Implementation-coupled** | "Verify database row created" | "Verify confirmation message displays" | Medium |
| **Placeholder data** | "Enter valid email" | "Enter 'test@example.com' in the email field" | Critical |
| **Duplicate tests** | "Add first item" vs "Add single item" | Consolidate to one comprehensive test | Low |
| **No error handling** | Only happy path tested | Add negative tests for invalid inputs | High |
| **Relative dates** | "Submit form yesterday" | "Submit form on 2024-01-15" | Medium |
| **No traceability** | Test has no @REQ-{id} tag | Add @REQ-{id} tag linking to AC | Medium |
| **Ambiguous elements** | "Click the button" | "Click the 'Submit Order' button" | High |

---

## Quality Scoring Rubric (100-point scale)

### Clarity (20 points)

| Score | Criteria |
|-------|----------|
| 16-20 | Precise language, no interpretation needed, specific UI elements always referenced |
| 11-15 | Mostly clear with minor ambiguities, occasionally generic |
| 6-10 | Some clarity but missing details, vague terms used |
| 1-5 | Ambiguous steps, no specific elements, cannot execute confidently |

**Score: {{clarity_score}}/20**

### Completeness (25 points)

| Score | Criteria |
|-------|----------|
| 19-25 | All scenarios covered (30/40/30 ratio), all edge categories, all preconditions/data/results |
| 13-18 | Good positive/negative coverage, most edge cases, minor gaps |
| 7-12 | Basic coverage, missing some edge cases or preconditions |
| 1-6 | Incomplete, missing test types or critical data |

**Score: {{completeness_score}}/25**

### Reproducibility (15 points)

| Score | Criteria |
|-------|----------|
| 13-15 | Any tester executes identically, all data explicit, environment documented |
| 9-12 | Reproducible with minor clarifications needed |
| 5-8 | Mostly reproducible, some undefined entities or relative references |
| 1-4 | Inconsistent execution, many assumptions |

**Score: {{reproducibility_score}}/15**

### Traceability (15 points)

| Score | Criteria |
|-------|----------|
| 13-15 | Bidirectional traceability, coverage matrix complete, all tags present |
| 9-12 | Most tests traced, coverage matrix mostly complete, few missing tags |
| 5-8 | Some traceability links, incomplete matrix, many missing tags |
| 1-4 | No traceability, no tags, no coverage mapping |

**Score: {{traceability_score}}/15**

### Independence (15 points)

| Score | Criteria |
|-------|----------|
| 13-15 | Fully standalone tests, explicit setup/teardown, can execute in any order |
| 9-12 | Mostly independent, setup mostly explicit, minor dependencies |
| 5-8 | Some dependencies between tests, assumptions about state |
| 1-4 | Heavy dependencies, hidden state, must execute in specific order |

**Score: {{independence_score}}/15**

### Atomicity (10 points)

| Score | Criteria |
|-------|----------|
| 10 | Single focused verification per test, one test condition per scenario |
| 7-9 | Mostly atomic, 2 verifications per test occasionally |
| 4-6 | Tests verify 3-4 conditions, some bloat |
| 1-3 | Tests verify 5+ conditions, too broad |

**Score: {{atomicity_score}}/10**

---

**TOTAL QUALITY SCORE: {{total_score}}/100**

### Quality Thresholds

| Range | Interpretation | Action |
|-------|----------------|--------|
| **90-100** | Production-ready | ‚úÖ Approve for execution |
| **75-89** | Acceptable with improvements | ‚ö†Ô∏è Minor revisions recommended |
| **60-74** | Requires revision | üîÑ Significant improvements needed |
| **<60** | Reject | ‚ùå Re-generate with corrections |

**Result**: {{quality_result}}

---

## Coverage Sufficiency Analysis

### Test Type Distribution

| Type | Count | Percentage | Target | Status | Gap |
|------|-------|------------|--------|--------|-----|
| **Positive** | {{positive_count}} | {{positive_pct}}% | 30% | {{positive_status}} | {{positive_gap}}% |
| **Negative** | {{negative_count}} | {{negative_pct}}% | 40% | {{negative_status}} | {{negative_gap}}% |
| **Edge** | {{edge_count}} | {{edge_pct}}% | 30% | {{edge_status}} | {{edge_gap}}% |
| **TOTAL** | {{total_tests}} | 100% | 100% | {{total_status}} | - |

**Legend**: ‚úÖ On target (¬±5%) | ‚ö†Ô∏è Close (¬±10%) | ‚ùå Off target (>10%)

### Edge Case Category Distribution

Based on production defect frequency research:

| Category | Count | Bug Frequency | Target | Status | Notes |
|----------|-------|---------------|--------|--------|-------|
| **Empty/Null** | {{empty_count}} | 32% of bugs | {{empty_target}} | {{empty_status}} | Null input, empty strings, zero counts |
| **Boundary** | {{boundary_count}} | 28% of bugs | {{boundary_target}} | {{boundary_status}} | Min/max values, off-by-one errors |
| **Error** | {{error_count}} | 22% of bugs | {{error_target}} | {{error_status}} | Timeouts, failures, malformed responses |
| **Concurrent** | {{concurrent_count}} | 12% of bugs | {{concurrent_target}} | {{concurrent_status}} | Race conditions, double-submit |
| **State** | {{state_count}} | 6% of bugs | {{state_target}} | {{state_status}} | Invalid transitions, back button |

**Recommendation**: {{edge_recommendation}}

### Industry Standards Comparison

| Coverage Level | Interpretation | Our Score | Comparison |
|----------------|----------------|-----------|------------|
| Below 60% | Insufficient‚Äîsignificant risk | {{our_coverage}}% | {{coverage_comparison}} |
| 60-70% | Minimum acceptable | {{our_coverage}}% | {{coverage_comparison}} |
| **70-80%** | **Industry standard** | {{our_coverage}}% | {{coverage_comparison}} |
| 80-90% | Strong coverage | {{our_coverage}}% | {{coverage_comparison}} |
| 90%+ | High coverage (safety-critical) | {{our_coverage}}% | {{coverage_comparison}} |
| 100% | Required by DO-178B, ISO 26262 | {{our_coverage}}% | {{coverage_comparison}} |

**Industry Benchmark**: 70-80% is the most commonly cited corporate gating standard.

**Our Status**: {{coverage_status}}

**Estimated Code Coverage** (based on AC mapping and test count): **{{estimated_coverage}}%**

---

## Recommendations

### Immediate Actions

{{immediate_actions}}

### Improvements for Next Iteration

{{improvement_recommendations}}

### Lessons for Future Test Generation

{{lessons_for_future}}

---

## Metadata

| Field | Value |
|-------|-------|
| Generated | {{date}} |
| Test Cases File | {{test_cases_filename}} |
| Total Test Cases | {{total_tests}} |
| Acceptance Criteria | {{ac_count}} |
| Quality Score | {{total_score}}/100 |
| Checklist Score | {{checklist_score}}/10 |
| Coverage Estimate | {{estimated_coverage}}% |
| Result | {{quality_result}} |
